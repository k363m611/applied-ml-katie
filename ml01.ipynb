Katie McGaughey: California Housing Price Prediction

Author: Katie McGaughey
Date: 3-15-2025
Objective: Predict the median house price in California using available housing features.

Introduction

This project uses the California housing dataset to predict house prices based on features such as median income, average number of rooms, and house age. We'll clean the data, train a linear regression model, and explore ways to improve performance.

Imports

In the code cell below, import the necessary Python libraries for this notebook.

# This is a Python cell
# All imports should be at the top of the notebook
# This cell will be executed when the notebook is loaded

# Import pandas for data manipulation and analysis (we might want to do more with it)
import pandas as pd

# Import pandas for data manipulation and analysis  (we might want to do more with it)
import numpy as np

# Import matplotlib for creating static visualizations
import matplotlib.pyplot as plt

# Import seaborn for statistical data visualization (built on matplotlib)
import seaborn as sns

# Import the California housing dataset from sklearn
from sklearn.datasets import fetch_california_housing

# Import train_test_split for splitting data into training and test sets
from sklearn.model_selection import train_test_split

# Import LinearRegression for building a linear regression model
from sklearn.linear_model import LinearRegression

# Import performance metrics for model evaluation
from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score
Section 1. Load and Explore the Data

1.1 Load the dataset and display the first 10 rows

Load the California housing dataset directly from scikit-learn.

The fetch_california_housing function returns a dictionary-like object with the data.
Convert it into a pandas DataFrame.
Display just the first 10 rows using head().
Example code:

data = fetch_california_housing(as_frame=True) data_frame = data.frame

data_frame.head(10)

# This is a Python cell. 
# Put your comments and code here. 
# Read the Markdown cell above for information and example code. 
#
# For example:

# Load the data
data = fetch_california_housing(as_frame=True)
data_frame = data.frame

# Might be large. Display just the first 10 rows (you can change this number)
data_frame.head(10)
MedInc	HouseAge	AveRooms	AveBedrms	Population	AveOccup	Latitude	Longitude	MedHouseVal
0	8.3252	41.0	6.984127	1.023810	322.0	2.555556	37.88	-122.23	4.526
1	8.3014	21.0	6.238137	0.971880	2401.0	2.109842	37.86	-122.22	3.585
2	7.2574	52.0	8.288136	1.073446	496.0	2.802260	37.85	-122.24	3.521
3	5.6431	52.0	5.817352	1.073059	558.0	2.547945	37.85	-122.25	3.413
4	3.8462	52.0	6.281853	1.081081	565.0	2.181467	37.85	-122.25	3.422
5	4.0368	52.0	4.761658	1.103627	413.0	2.139896	37.85	-122.25	2.697
6	3.6591	52.0	4.931907	0.951362	1094.0	2.128405	37.84	-122.25	2.992
7	3.1200	52.0	4.797527	1.061824	1157.0	1.788253	37.84	-122.25	2.414
8	2.0804	42.0	4.294118	1.117647	1206.0	2.026891	37.84	-122.26	2.267
9	3.6912	52.0	4.970588	0.990196	1551.0	2.172269	37.84	-122.25	2.611
1.2 Check for missing values and display summary statistics

In the cell below:

Use info() to check data types and missing values.
Use describe() to see summary statistics.
Use isnull().sum() to identify missing values in each column.
Example code:

data_frame.info()

data_frame.describe()

data_frame.isnull().sum()

# This is a Python cell. 
# Put your comments and code here.
# Read the Markdown cell above for information and example code. 

data_frame.info()

data_frame.describe()

data_frame.isnull().sum()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 9 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   MedInc       20640 non-null  float64
 1   HouseAge     20640 non-null  float64
 2   AveRooms     20640 non-null  float64
 3   AveBedrms    20640 non-null  float64
 4   Population   20640 non-null  float64
 5   AveOccup     20640 non-null  float64
 6   Latitude     20640 non-null  float64
 7   Longitude    20640 non-null  float64
 8   MedHouseVal  20640 non-null  float64
dtypes: float64(9)
memory usage: 1.4 MB
MedInc         0
HouseAge       0
AveRooms       0
AveBedrms      0
Population     0
AveOccup       0
Latitude       0
Longitude      0
MedHouseVal    0
dtype: int64
Analysis:

How many data instances (also called data records or data rows) are there? 20640

How many features (also columns or attributes) are there? 9

What are the names of the features? ("Feature" is used most often in ML projects.) MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude, MedHouseVal

Which features are numeric? All are Float and all seem numeric

Which features are categorical (non-numeric)? None

Are there any missing values? How should they be handled? Should we delete a sparsely populated column? Delete an incomplete data row? Substitute with a different value? It depends on what you and your team has discussed. If it doesn't seemt to affect the visualizations and/or insights in anyway then you could possibly leave them or delete them. You may also want to substitute it. It is completely situation dependent.

What else do you notice about the dataset? Are there any data issues? I do not notice any data issues.

Section 2. Visualize Feature Distributions

2.1 Create histograms, boxplots, and scatterplots

Create histograms for all numeric features using data_frame.hist() with 30 bins.
Create a boxenplots using sns.boxenplot().
Create scatter plots using sns.pairplot().
First, histograms

Generate histograms for all numerical columns

Example code:

data_frame.hist(bins=30, figsize=(12, 8))

plt.show()

# This is a Python cell. 
# Put your comments and code here.

# Generate histograms for all numerical columns
data_frame.hist(bins=30, figsize=(12, 8))

plt.show()

Generate one Boxenplot for each column (good for large datasets)

Example code:

for column in data_frame.columns: plt.figure(figsize=(6, 4)) sns.boxenplot(data=data_frame[column]) plt.title(f'Boxenplot for {column}') plt.show()

# This is a Python cell. 
# Put your comments and code here.
# Generate a boxenplot for each column
for column in data_frame.columns:
    plt.figure(figsize=(6, 4))
    sns.boxenplot(data=data_frame[column])
    plt.title(f'Boxenplot for {column}')
    plt.show()









Third - Scatter Plots

Generate all Scatter plots (there is a LOT of data, so this will take a while)

Comment out after analysis to speed up the notebook.

Example code:

sns.pairplot(data_frame)

plt.show()

# This is a Python cell. 
# Put your comments and code here.

# Generate all scatter plots
sns.pairplot(data_frame)

plt.show()

Section 3. Feature Selection and Justification

3.1 Choose two input features for predicting the target

Select MedInc and AveRooms as predictors.
Select MedHouseVal as the target variable.
In the following, X is capitalized because it represents a matrix (consistent with mathematical notation). y is lowercase because it represents a vector (consistent with mathematical notation).

First:

Create a list of contributing features and the target variable
Define the target feature string (the variable we want to predict)
Define the input DataFrame
Define the output DataFrame
Example code:

features: list = ['MedInc', 'AveRooms']

target: str = 'MedHouseVal'

df_X = data_frame[features]

df_y = data_frame[target]

# This is a Python cell. 
# Put your comments and code here.

# Create a list of contributing features and the target variable
features: list = ['MedInc', 'AveRooms']

target: str = 'MedHouseVal'

df_X = data_frame[features]

df_y = data_frame[target]

print(df_X)
print(df_y)
       MedInc  AveRooms
0      8.3252  6.984127
1      8.3014  6.238137
2      7.2574  8.288136
3      5.6431  5.817352
4      3.8462  6.281853
...       ...       ...
20635  1.5603  5.045455
20636  2.5568  6.114035
20637  1.7000  5.205543
20638  1.8672  5.329513
20639  2.3886  5.254717

[20640 rows x 2 columns]
0        4.526
1        3.585
2        3.521
3        3.413
4        3.422
         ...  
20635    0.781
20636    0.771
20637    0.923
20638    0.847
20639    0.894
Name: MedHouseVal, Length: 20640, dtype: float64
Section 4. Train a Linear Regression Model

4.1 Split the data

Split the dataset into training and test sets (80% train / 20% test).

Call train_test_split() by passing in:

df_X – Feature matrix (input data) as a pandas DataFrame
y – Target values as a pandas Series
test_size – Fraction of data to use for testing (e.g., 0.1 = 10%)
random_state – Seed value for reproducible splits
We'll get back four return values:

X_train – Training set features (DataFrame)
X_test – Test set features (DataFrame)
y_train – Training set target values (Series)
y_test – Test set target values (Series)
Example code:

X_train, X_test, y_train, y_test = train_test_split( df_X, df_y, test_size=0.2, random_state=42)

# This is a Python cell. 
# Put your comments and code here.

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    df_X, df_y, test_size=0.2, random_state=42)

print(X_train)
print(X_test)
print(y_train)
print(y_test)
       MedInc  AveRooms
14196  3.2596  5.017657
8267   3.8125  4.473545
17445  4.1563  5.645833
14265  1.9425  4.002817
2271   3.5542  6.268421
...       ...       ...
11284  6.3700  6.129032
11964  3.0500  6.868597
5390   2.9344  3.986717
860    5.7192  6.395349
15795  2.5755  3.402576

[16512 rows x 2 columns]
       MedInc  AveRooms
20046  1.6812  4.192201
3024   2.5313  5.039384
15663  3.4801  3.977155
20484  5.7376  6.163636
9814   3.7250  5.492991
...       ...       ...
15362  4.6050  7.002212
16623  2.7266  6.131915
18086  9.2298  7.237676
2144   2.7850  5.289030
3665   3.5521  3.988839

[4128 rows x 2 columns]
14196    1.030
8267     3.821
17445    1.726
14265    0.934
2271     0.965
         ...  
11284    2.292
11964    0.978
5390     2.221
860      2.835
15795    3.250
Name: MedHouseVal, Length: 16512, dtype: float64
20046    0.47700
3024     0.45800
15663    5.00001
20484    2.18600
9814     2.78000
          ...   
15362    2.63300
16623    2.66800
18086    5.00001
2144     0.72300
3665     1.51500
Name: MedHouseVal, Length: 4128, dtype: float64
4.2 Train the model

Create and fit a LinearRegression model.

LinearRegression – A class from sklearn.linear_model that creates a linear regression model.

model – An instance of the LinearRegression model. This object will store the learned coefficients and intercept after training.

fit() – Trains the model by finding the best-fit line for the training data using the Ordinary Least Squares (OLS) method.

X_train – The input features used to train the model.

y_train – The target values used to train the model.

Example code:

model = LinearRegression()

model.fit(X_train, y_train)

# This is a Python cell. 
# Put your comments and code here.

# Create and fit linear regression model

model = LinearRegression()

model.fit(X_train, y_train)
LinearRegression()
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
Make predictions for the test set.

The model.predict() method applies this equation to the X test data to compute predicted values.

y_pred = model.predict(X_test)

y_pred contains all the predicted values for all the rows in X_test based on the linear regression model.

Example code:

y_pred = model.predict(X_test)

# This is a Python cell. 
# Put your comments and code here.

# Make predictions for test set
y_pred = model.predict(X_test)

print(y_pred)
[1.16230214 1.49913482 1.95573059 ... 4.33311942 1.59978552 1.98667198]
4.3 Report R^2, MAE, RMSE

Evaluate the model using R^2, MAE, and RMSE.

First:

Coefficient of Determination (R^2) - This tells you how well the model explains the variation in the target variable. A value close to 1 means the model fits the data well; a value close to 0 means the model doesn’t explain the variation well.
Example code:

r2 = r2_score(y_test, y_pred)

print(f'R²: {r2:.2f}')

# This is a Python cell. 
# Put your comments and code here.

# Evaluate the model with R^2
r2 = r2_score(y_test, y_pred)

print(f'R²: {r2:.2f}')
R²: 0.46
Second:

Mean Absolute Error (MAE) - This is the average of the absolute differences between the predicted values and the actual values. A smaller value means the model’s predictions are closer to the actual values.
Example code:

mae = mean_absolute_error(y_test, y_pred)

print(f'MAE: {mae:.2f}')

# This is a Python cell. 
# Put your comments and code here.

# Evaluate the model with MAE
mae = mean_absolute_error(y_test, y_pred)

print(f'MAE: {mae:.2f}')
MAE: 0.62
Third:

Root Mean Squared Error (RMSE) - This is the square root of the average of the squared differences between the predicted values and the actual values. It gives a sense of how far the predictions are from the actual values, with larger errors having more impact.
Example code:

rmse = root_mean_squared_error(y_test, y_pred)

print(f'RMSE: {rmse:.2f}')

# This is a Python cell. 
# Put your comments and code here.

# Evalute the model with RMSE
rmse = root_mean_squared_error(y_test, y_pred)

print(f'RMSE: {rmse:.2f}') 
RMSE: 0.84
